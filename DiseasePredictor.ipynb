{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337ba3c5-a235-47ee-910c-cea00a82913d",
   "metadata": {},
   "source": [
    "### 1. Loading the disease–symptom matrix\n",
    "\n",
    "This step reads the `symptom_matrix.csv` file into a pandas DataFrame.  \n",
    "Each row represents a disease, the `disease` column holds the disease name, and all other columns are symptoms encoded as 0/1 flags.  \n",
    "\n",
    "We then:\n",
    "- Extract a list of disease names (`diseases`).\n",
    "- Collect the symptom column names (`symptom_cols`), excluding the `disease` column.\n",
    "- Convert all symptom columns to integer type to guarantee numeric 0/1 values.\n",
    "- Build `symptom_matrix`, a NumPy array of shape `(num_diseases, num_symptoms)`, which we later use as the “canonical” symptom vector for each disease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65aa3a20-34ad-4416-8dd6-3e7ba7641b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from numpy.linalg import norm\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "920fc049-b87f-4178-bcba-988b7274ce75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symptom_matrix dtype: int64\n",
      "num diseases: 246945 num symptoms: 377\n",
      "Symptoms with at least one positive: 328 out of 377\n",
      "New Y_symptoms shape: (247479, 328)\n"
     ]
    }
   ],
   "source": [
    "# ----- 1.1 Load symptom matrix -----\n",
    "df_symptoms = pd.read_csv(\"DiseaseAndSymptoms.csv\")\n",
    "\n",
    "# Disease names (strings)\n",
    "diseases = df_symptoms[\"diseases\"].tolist()\n",
    "\n",
    "# Symptom columns (all numeric)\n",
    "symptom_cols = [c for c in df_symptoms.columns if c != \"diseases\"]\n",
    "df_symptoms[symptom_cols] = df_symptoms[symptom_cols].astype(int)\n",
    "\n",
    "# Numeric matrix (rows = diseases, cols = symptoms)\n",
    "symptom_matrix = df_symptoms[symptom_cols].values\n",
    "print(\"symptom_matrix dtype:\", symptom_matrix.dtype)\n",
    "print(\"num diseases:\", len(diseases), \"num symptoms:\", len(symptom_cols))\n",
    "\n",
    "pos_counts = Y_symptoms.sum(axis=0)          # sum over rows\n",
    "keep_mask = pos_counts > 0                   # True where there is at least one 1\n",
    "print(\"Symptoms with at least one positive:\", keep_mask.sum(), \"out of\", len(symptom_cols))\n",
    "\n",
    "# 2) Filter symptom columns and matrices\n",
    "symptom_cols = [c for c, keep in zip(symptom_cols, keep_mask) if keep]\n",
    "Y_symptoms = Y_symptoms[:, keep_mask]\n",
    "symptom_matrix = symptom_matrix[:, keep_mask]\n",
    "\n",
    "print(\"New Y_symptoms shape:\", Y_symptoms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427a8b1-2285-4f17-9d74-a5b85c67fe8d",
   "metadata": {},
   "source": [
    "### 2. Creating synthetic training texts from the matrix\n",
    "\n",
    "Here we generate synthetic training data directly from the disease–symptom matrix.  \n",
    "\n",
    "For each disease row:\n",
    "- We find all symptoms whose value is 1 (i.e., symptoms that are present for that disease).\n",
    "- We build a simple text description like `\"patient has fever, runny nose, dry throat\"` from those symptom names.\n",
    "- We append this synthetic text to `texts`.\n",
    "- We append the corresponding 0/1 symptom vector (the row from `symptom_matrix`) to `symptom_labels`.\n",
    "\n",
    "The goal is to create a first batch of training examples where the mapping from text → symptom vector is perfectly clean (because the text is literally generated from the symptom vector).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93cf99d0-3c89-4fc3-8c2d-f8d27c5460ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic examples: 246945\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----- 1.2 Synthetic texts from symptom matrix -----\n",
    "texts = []\n",
    "symptom_labels = []\n",
    "\n",
    "for i, disease_vec in enumerate(symptom_matrix):\n",
    "    symptoms_present = [symptom_cols[j] for j in range(len(symptom_cols)) \n",
    "                        if disease_vec[j] == 1]\n",
    "    if symptoms_present:\n",
    "        text = \"patient has \" + \", \".join(s.replace(\"_\", \" \") for s in symptoms_present)\n",
    "        texts.append(text)\n",
    "        symptom_labels.append(disease_vec.astype(int))\n",
    "\n",
    "print(\"Synthetic examples:\", len(texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0378bae-6864-405c-ab11-ac080cbaed81",
   "metadata": {},
   "source": [
    "### 3. Adding real free‑text JSONL data (train + test)\n",
    "\n",
    "In this step we load the real free‑text dataset stored as JSONL files (`train.jsonl` and `test.jsonl`).  \n",
    "Each line in these files contains an object with:\n",
    "\n",
    "- `input_text`: a natural‑language symptom description written in a more realistic style.\n",
    "- `output_text`: the disease name corresponding to that description.\n",
    "\n",
    "We then:\n",
    "\n",
    "1. Load both JSONL files and concatenate them into a single list `real_data`.\n",
    "2. Build a dictionary `disease_to_idx` that maps each disease name in the symptom matrix to its row index.\n",
    "3. For each item in `real_data`:\n",
    "   - We read the disease name (`output_text`) and symptom description (`input_text`).\n",
    "   - If the disease exists in our matrix, we fetch the corresponding 0/1 symptom row.\n",
    "   - We append the free‑text symptom description to `texts`.\n",
    "   - We append the disease’s symptom row to `symptom_labels`.\n",
    "   - If the disease name does not exist in the matrix, we record it in `unmatched` and skip it.\n",
    "\n",
    "At the end, we combine the synthetic texts and real JSONL texts into `X_text`, and the corresponding symptom vectors into `Y_symptoms`.  \n",
    "This gives us training pairs of the form: **(free text or synthetic text) → symptom vector** in the same 0/1 space as our disease–symptom matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88e4c592-70c9-4099-a68d-5081be66bab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real JSONL rows: 1065\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(filename):\n",
    "    data = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "real_data = load_jsonl(\"train.jsonl\") + load_jsonl(\"test.jsonl\")\n",
    "print(\"Real JSONL rows:\", len(real_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ecfdb-cc64-40b0-b87c-58602105d010",
   "metadata": {},
   "source": [
    "### 4. Filtering out symptom labels that never appear\n",
    "\n",
    "Some symptom columns in `Y_symptoms` may be all zeros — that is, no training example ever has that symptom as present.  \n",
    "\n",
    "Logistic regression cannot be trained on a label that has only one class (all 0), so we:\n",
    "\n",
    "- Compute `pos_counts`, the sum of each symptom column across all training examples.\n",
    "- Build `keep_mask`, a boolean mask that is `True` for symptoms that have at least one positive label.\n",
    "- Filter:\n",
    "  - `symptom_cols` to keep only those symptoms with at least one positive.\n",
    "  - `Y_symptoms` to keep only those columns.\n",
    "  - `symptom_matrix` to keep the same subset of symptoms.\n",
    "\n",
    "This ensures every remaining symptom label has both 0 and 1 examples in the training set, making the multi‑label classifier trainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "229502e9-29e0-4e3a-8cd3-dbc7d4d071c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 1599\n",
      "Unmatched diseases (skipped): 11\n",
      "Y_symptoms dtype: int64 shape: (248013, 328)\n"
     ]
    }
   ],
   "source": [
    "# Map disease name -> index (from symptom matrix)\n",
    "disease_to_idx = {d: i for i, d in enumerate(diseases)}\n",
    "\n",
    "unmatched = set()\n",
    "\n",
    "for item in real_data:\n",
    "    disease_name = item[\"output_text\"]\n",
    "    symptom_text = item[\"input_text\"]\n",
    "    if disease_name in disease_to_idx:\n",
    "        idx = disease_to_idx[disease_name]\n",
    "        row_vec = symptom_matrix[idx].astype(int)\n",
    "        texts.append(symptom_text)\n",
    "        symptom_labels.append(row_vec)\n",
    "    else:\n",
    "        unmatched.add(disease_name)\n",
    "\n",
    "print(\"Total training examples:\", len(texts))\n",
    "print(\"Unmatched diseases (skipped):\", len(unmatched))\n",
    "\n",
    "# Convert to arrays\n",
    "X_text = np.array(texts)\n",
    "Y_symptoms = np.array(symptom_labels)\n",
    "print(\"Y_symptoms dtype:\", Y_symptoms.dtype, \"shape:\", Y_symptoms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff0157-cfc2-44e8-bf30-75e3ca367b47",
   "metadata": {},
   "source": [
    "### 5. Training a multi‑label text → symptom classifier (TF‑IDF + Logistic Regression)\n",
    "\n",
    "Here we train the core NLP model that predicts symptoms from text.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. **Train–test split**  \n",
    "   We split `X_text` and `Y_symptoms` into training and test sets using `train_test_split`. This allows us to measure how well the model generalises.\n",
    "\n",
    "2. **Text vectorisation (TF‑IDF)**  \n",
    "   We create a `TfidfVectorizer` with:\n",
    "   - A maximum vocabulary size (e.g. 5000 features).\n",
    "   - Unigrams and bigrams (`ngram_range=(1, 2)`).\n",
    "   - A minimum document frequency (`min_df=2`) to drop extremely rare tokens.\n",
    "\n",
    "   We fit the vectorizer on the training texts and transform both train and test texts into TF‑IDF feature matrices (`X_train_vec`, `X_test_vec`).  \n",
    "   This is the NLP featurisation step: raw text → numeric feature vectors.\n",
    "\n",
    "3. **Multi‑label classifier**  \n",
    "   We wrap `LogisticRegression` inside `MultiOutputClassifier`.  \n",
    "   This creates one binary logistic regression model per symptom, so the model learns `P(symptom_k = 1 | text)` for every symptom `k`.\n",
    "\n",
    "4. **Training and evaluation**  \n",
    "   We fit the multi‑output classifier on the TF‑IDF features and `Y_train`.  \n",
    "   Then we predict on the test set and compute the micro‑averaged F1 score, giving a rough measure of how well the model predicts symptom vectors from text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e626708-4ca2-458b-a2ad-9c04cc3b8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diseases                            object\n",
      "anxiety and nervousness              int64\n",
      "depression                           int64\n",
      "shortness of breath                  int64\n",
      "depressive or psychotic symptoms     int64\n",
      "                                     ...  \n",
      "hip weakness                         int64\n",
      "back swelling                        int64\n",
      "ankle stiffness or tightness         int64\n",
      "ankle weakness                       int64\n",
      "neck weakness                        int64\n",
      "Length: 378, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_symptoms.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f09205f3-961a-47c8-9fd6-3f7c52b8bb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 0.9976008831476716\n",
      "Saved vectorizer, model, metadata.\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_text, Y_symptoms, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Multi-label classifier\n",
    "clf = MultiOutputClassifier(\n",
    "    LogisticRegression(max_iter=500, random_state=42)\n",
    ")\n",
    "clf.fit(X_train_vec, Y_train)\n",
    "\n",
    "# Quick evaluation\n",
    "Y_pred = clf.predict(X_test_vec)\n",
    "print(\"Micro F1:\", f1_score(Y_test, Y_pred, average=\"micro\"))\n",
    "\n",
    "# Save artifacts\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "joblib.dump(clf, \"symptom_model.pkl\")\n",
    "joblib.dump({\"diseases\": diseases, \"symptom_cols\": symptom_cols}, \"metadata.pkl\")\n",
    "print(\"Saved vectorizer, model, metadata.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c68f9d03-b29e-46f5-b19a-5767db01194c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharp abdominal pain 32307\n",
      "vomiting 27923\n",
      "headache 24816\n",
      "cough 24442\n",
      "sharp chest pain 24016\n",
      "nausea 23735\n",
      "back pain 21857\n",
      "shortness of breath 21346\n",
      "fever 20541\n",
      "dizziness 17362\n",
      "abnormal appearing skin 16573\n",
      "nasal congestion 16249\n",
      "leg pain 16239\n",
      "skin swelling 15315\n",
      "depressive or psychotic symptoms 15064\n",
      "lower abdominal pain 14984\n",
      "sore throat 14005\n",
      "burning abdominal pain 12981\n",
      "skin rash 12473\n",
      "skin lesion 12440\n",
      "arm pain 11619\n",
      "weakness 11598\n",
      "low back pain 11053\n",
      "ear pain 10633\n",
      "depression 10556\n",
      "side pain 10554\n",
      "itching of skin 10511\n",
      "diarrhea 10472\n",
      "loss of sensation 10399\n",
      "skin growth 10311\n"
     ]
    }
   ],
   "source": [
    "counts = Y_symptoms.sum(axis=0)\n",
    "for s, c in sorted(zip(symptom_cols, counts), key=lambda x: -x[1])[:30]:\n",
    "    print(s, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a1441-7ce6-4690-9d28-00ae72ab6e0a",
   "metadata": {},
   "source": [
    "### 6. Predicting diseases via symptom similarity\n",
    "\n",
    "The inference pipeline in the original approach works in two stages:\n",
    "\n",
    "1. **Text → symptom probabilities**  \n",
    "   - Given a new user text, we transform it with the trained TF‑IDF vectorizer.\n",
    "   - For each internal logistic regression estimator (one per symptom), we call `predict_proba` to obtain `P(symptom_k = 1 | text)`.\n",
    "   - We stack these probabilities into a vector `symptom_probs`, which represents the model’s belief about which symptoms the user has.\n",
    "\n",
    "2. **Symptom probabilities → disease similarity scores**  \n",
    "   - For each disease row in `symptom_matrix`, we compute the cosine similarity between:\n",
    "     - `symptom_probs` (probabilities from the text model).\n",
    "     - The disease’s 0/1 symptom vector.\n",
    "\n",
    "   - Higher cosine similarity means the pattern of predicted symptoms is more similar to that disease’s canonical symptom profile.\n",
    "   - We sort diseases by similarity and return the top‑k as candidate diagnoses, with similarity scores.\n",
    "\n",
    "This implements the idea of “scan the text for symptoms, then choose diseases whose symptom vectors most closely match what the model extracted.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57f89a7a-89da-4079-bf8e-277416202ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug reaction: 0.81\n",
      "impetigo: 0.79\n",
      "viral exanthem: 0.78\n",
      "viral exanthem: 0.78\n",
      "actinic keratosis: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "def cosine_similarity(u, v):\n",
    "    if norm(u) == 0 or norm(v) == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(u, v) / (norm(u) * norm(v)))\n",
    "\n",
    "def predict_diseases_from_text(\n",
    "    text,\n",
    "    vectorizer,\n",
    "    model,\n",
    "    symptom_matrix,\n",
    "    diseases,\n",
    "    symptom_cols,\n",
    "    top_k=5\n",
    "):\n",
    "    # 1. text -> symptom probabilities\n",
    "    X_vec = vectorizer.transform([text])\n",
    "    probs_per_symptom = []\n",
    "    for est in model.estimators_:\n",
    "        probs = est.predict_proba(X_vec)[:, 1]  # prob of label=1\n",
    "        probs_per_symptom.append(probs[0])\n",
    "    symptom_probs = np.array(probs_per_symptom)  # (num_symptoms,)\n",
    "\n",
    "    # >>> BOOST EXACT MATCHES HERE <<<\n",
    "    symptom_probs = boost_exact_matches(text, symptom_probs, symptom_cols, boost=0.7)\n",
    "\n",
    "    # 2. similarity to each disease row\n",
    "    scores = []\n",
    "    for i, disease_vec in enumerate(symptom_matrix):\n",
    "        score = cosine_similarity(symptom_probs, disease_vec.astype(float))\n",
    "        scores.append((diseases[i], score))\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:top_k]\n",
    "\n",
    "\n",
    "# Reload artifacts (simulate real usage)\n",
    "vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "model = joblib.load(\"symptom_model.pkl\")\n",
    "metadata = joblib.load(\"metadata.pkl\")\n",
    "diseases = metadata[\"diseases\"]\n",
    "symptom_cols = metadata[\"symptom_cols\"]\n",
    "symptom_matrix = df_symptoms[symptom_cols].values\n",
    "\n",
    "# Test\n",
    "user_text = \"I have a runny nose, dry throat and a temperature for two days\"\n",
    "results = predict_diseases_from_text(user_text, vectorizer, model, symptom_matrix, diseases, symptom_cols, top_k=5)\n",
    "\n",
    "for disease, score in results:\n",
    "    print(f\"{disease}: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ef6dc-6cb7-4bad-96fa-49f8af46a6c1",
   "metadata": {},
   "source": [
    "### 7. Inspecting the model’s predicted symptoms\n",
    "\n",
    "To understand why disease predictions looked wrong, we added a debugging step that:\n",
    "\n",
    "- Takes `symptom_probs` (the probabilities for each symptom).\n",
    "- Pairs each probability with its symptom name.\n",
    "- Sorts these pairs in descending order of probability.\n",
    "- Prints the top‑N symptoms and their probabilities.\n",
    "\n",
    "For example, for the input `\"I have a runny nose, dry throat and a temperature for two days\"`, we observed that the model assigned relatively high probabilities to skin‑related symptoms (e.g. abnormal appearing skin, itching of skin) and very low probabilities to respiratory symptoms such as runny nose, sore throat, and coryza.\n",
    "\n",
    "This revealed that the text → symptom model was not learning the intuitive mapping between common respiratory symptom phrases and their corresponding symptom labels, but was heavily biased toward skin and pain symptoms that are frequent in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6ca6a27-2bca-4c0b-98b5-99ad3a76acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boost_exact_matches(text, symptom_probs, symptom_cols, boost=0.5):\n",
    "    text_low = text.lower()\n",
    "    for i, s in enumerate(symptom_cols):\n",
    "        phrase = s.replace(\"_\", \" \")\n",
    "        if phrase in text_low:\n",
    "            symptom_probs[i] = max(symptom_probs[i], boost)\n",
    "    return symptom_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d4df7-553d-4f83-a76a-ef65ba6ee2c1",
   "metadata": {},
   "source": [
    "### 9. Training a direct text → disease classifier (new approach)\n",
    "\n",
    "In the new, simpler approach we ignore the symptom layer at prediction time and train a model directly from text to disease.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. **Build training data**  \n",
    "   - Inputs: `input_text` from the JSONL files (free‑text symptom descriptions).\n",
    "   - Labels: `output_text` from the same files (disease names).\n",
    "\n",
    "2. **TF‑IDF features**  \n",
    "   - We use `TfidfVectorizer` to convert each input text into a numeric feature vector, similar to the previous model.\n",
    "\n",
    "3. **Multi‑class Logistic Regression**  \n",
    "   - We train a single `LogisticRegression` classifier to predict the disease name for each text.\n",
    "   - This is a standard multi‑class text classification setup: one label per example.\n",
    "\n",
    "4. **Inference**  \n",
    "   - For a new user text, we transform it with the vectorizer, call `predict_proba`, and obtain a probability distribution over all diseases in the training set.\n",
    "   - We sort diseases by predicted probability and display the top‑k, with percentages, as the “similarity” or model confidence.\n",
    "\n",
    "This new model directly optimises for the final target (disease) rather than going through an intermediate symptom space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "363c417b-c6f5-4828-b9d0-cc0421feacaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9014084507042254\n"
     ]
    }
   ],
   "source": [
    "# Training data from JSONL only\n",
    "texts = [item[\"input_text\"] for item in real_data]\n",
    "labels = [item[\"output_text\"] for item in real_data]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "vec = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=2)\n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "X_test_vec = vec.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "joblib.dump(vec, \"text_vec.pkl\")\n",
    "joblib.dump(clf, \"text_disease_model.pkl\")\n",
    "print(\"Accuracy:\", (clf.predict(X_test_vec) == y_test).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c746605c-496e-4644-acbb-79538be73d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allergy: 31.5%\n",
      "common cold: 9.1%\n",
      "drug reaction: 4.8%\n",
      "gastroesophageal reflux disease: 4.5%\n",
      "diabetes: 4.4%\n"
     ]
    }
   ],
   "source": [
    "def predict_diseases_text_only(text, vec, clf, top_k=5):\n",
    "    X_vec = vec.transform([text])\n",
    "    probs = clf.predict_proba(X_vec)[0]\n",
    "    classes = clf.classes_\n",
    "    pairs = sorted(zip(classes, probs), key=lambda x: x[1], reverse=True)\n",
    "    return pairs[:top_k]\n",
    "\n",
    "\n",
    "user_text = \"I have a runny nose, dry throat and a temperature for two days\"\n",
    "for disease, p in predict_diseases_text_only(user_text, vec, clf):\n",
    "    print(f\"{disease}: {p:.1%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f79a0b-ac53-4811-861d-ae679e81ec78",
   "metadata": {},
   "source": [
    "### 10. Why the original text → symptoms → disease pipeline gave bad results, and how the new text → disease model fixes it\n",
    "\n",
    "In the original design, the pipeline had two stages:\n",
    "\n",
    "1. **Text → symptoms (multi‑label)**  \n",
    "   - The model tried to predict hundreds of symptom labels from each text.\n",
    "   - Training labels came from a mix of:\n",
    "     - Synthetic texts generated directly from the disease–symptom matrix.\n",
    "     - Real JSONL texts mapped to symptom vectors via disease names.\n",
    "   - The label distribution was heavily imbalanced: many pain and skin‑related symptoms appeared extremely frequently, while common respiratory symptoms (runny nose, sore throat, coryza) were relatively under‑represented.\n",
    "\n",
    "2. **Symptoms → disease (similarity)**  \n",
    "   - The model’s predicted symptom probabilities were compared to each disease’s symptom row using cosine similarity.\n",
    "   - Diseases whose symptom vectors aligned best with the (possibly biased) symptom probabilities were ranked highest.\n",
    "\n",
    "Because the text → symptom model was trained on noisy, imbalanced data, it often predicted high probabilities for skin and pain symptoms even when the text clearly described respiratory symptoms. When these biased symptom vectors were fed into the similarity step, the system naturally preferred diseases with strong skin components (e.g. impetigo, drug reaction, actinic keratosis) even for inputs like “runny nose, dry throat, temperature”. In short, **errors at the symptom layer propagated and were amplified by the similarity step**.\n",
    "\n",
    "The new approach removes this fragile intermediate step:\n",
    "\n",
    "- Instead of predicting hundreds of symptoms, we train a **single multi‑class classifier** that maps text directly to diseases, using the JSONL data (`input_text → output_text`) as the supervision signal.\n",
    "- TF‑IDF + Logistic Regression now optimises exactly the quantity we care about (disease label) and uses the best‑quality labels we have (disease names from the JSONL set), without having to infer a large, noisy symptom vector first.\n",
    "- At inference, a single `predict_proba` call yields a probability distribution over diseases, and we interpret these probabilities as similarity/confidence scores.\n",
    "\n",
    "As a result, for the same input “I have a runny nose, dry throat and a temperature for two days”, the direct text → disease model sensibly ranks **allergy** and **common cold** highest, instead of unrelated skin conditions. The model is simpler, less error‑prone, and more closely aligned with the available data and the end goal of suggesting likely diseases from free‑text symptom descriptions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
